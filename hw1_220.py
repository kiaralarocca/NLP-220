# -*- coding: utf-8 -*-
"""hw1_220.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/11jEnchw6ueTS2zay_eDmTZpMk9kugfSy

Kiara LaRocca

NLP 220 | Assignment 1

October 25, 2024

This is Part A of Assignment 1. In this part, we create 3 classifiers using 3 different core features. They are then evaluated on a test data set.
"""

# Import packages and libraries
import pandas as pd
import seaborn as sns
import spacy
from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer
from sklearn.naive_bayes import MultinomialNB
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, f1_score
import matplotlib.pyplot as plt
from sklearn.svm import SVC
from sklearn.tree import DecisionTreeClassifier

# Download and load spacy
#!python -m spacy download en_core_web_sm
spacy = spacy.load("en_core_web_sm")

# Mount Colab in Google Drive for access to data and other files
from google.colab import drive
drive.mount('/content/drive/')

# Read the CSV file from Google Drive
file_path = '/content/drive/My Drive/small_books_rating.csv'
df = pd.read_csv(file_path)

# Concatenate review/summary and review/text as one column named 'combined' and apply lower() to the whole column
df['combined'] = (df['review/summary'].fillna('') + ' ' + df['review/text'].fillna('')).str.lower()

# Display the first few rows
df.head()

# Score Distributions
df['review/score'].value_counts()

# Create the 'label' column in the existing dataframe
df['label'] = df['review/score'].apply(lambda x: 1 if x >= 4 else (0 if x <= 2 else None))

# Remove rows where 'review/score' is 3
df = df.dropna(subset=['label'])

# Convert 'label' column to integer type
df['label'] = df['label'].astype(int)

# Display the updated df
df.head()

# Binary Class Distribution
df['label'].value_counts()

# Visualize
fig, ax = plt.subplots(1, 2, figsize=(12, 6))

# Train set class distribution
df['label'].value_counts().plot(kind='bar', ax=ax[0], title='Training Set Class Distribution', color=['skyblue', 'orange'])
ax[0].set_xticks([0, 1])
ax[0].set_xticklabels(['Positive', 'Negative'], rotation=0)
ax[0].set_ylabel('Count')

# Test set class distribution
df['label'].value_counts().plot(kind='bar', ax=ax[1], title='Test Set Class Distribution', color=['skyblue', 'orange'])
ax[1].set_xticks([0, 1])
ax[1].set_xticklabels(['Positive', 'Negative'], rotation=0)
ax[1].set_ylabel('Count')

plt.tight_layout()
plt.show()

# List to store model names and confusion matrices
models = []
confusion_matrices = []

# Create features and labels
x = df['combined']
y = df['label']

# Train-test split
x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.15, random_state=42)

# Naive Bayes classifier with Bag of Words (BoW)
vectorizer_bow = CountVectorizer()
x_train_bow = vectorizer_bow.fit_transform(x_train)
x_test_bow = vectorizer_bow.transform(x_test)

nb_bow = MultinomialNB()
nb_bow.fit(x_train_bow, y_train)
y_pred_bow = nb_bow.predict(x_test_bow)

print("Naive Bayes with BoW:")
print(classification_report(y_test, y_pred_bow))

# Accuracy
accuracy = accuracy_score(y_test, y_pred_bow)

# F1 Score
f1 = f1_score(y_test, y_pred_bow)

# Calculate Macro F1 Score
macro_f1 = f1_score(y_test, y_pred_bow, average='macro')

# Print results
print(f"Accuracy: {accuracy:.4f}")
print(f"F1 Score: {f1:.4f}")
print(f"Macro F1 Score: {macro_f1:.4f}")

# Calculate the confusion matrix
conf_matrix = confusion_matrix(y_test, y_pred_bow)

# Print the confusion matrix
print("Confusion Matrix:")

# Create a heatmap using Seaborn
plt.figure(figsize=(6, 4))
sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=["Negative", "Positive"], yticklabels=["Negative", "Positive"])

# Set labels and title
plt.title('Confusion Matrix')
plt.xlabel('Predicted Label')
plt.ylabel('True Label')

# Display the heatmap
plt.show()

# Store results
models.append("Naive Bayes with BoW")
confusion_matrices.append(confusion_matrix(y_test, y_pred_bow))

# Naive Bayes Classifier with TF-IDF
vectorizer_tfidf = TfidfVectorizer()
x_train_tfidf = vectorizer_tfidf.fit_transform(x_train)
x_test_tfidf = vectorizer_tfidf.transform(x_test)

nb_tfidf = MultinomialNB()
nb_tfidf.fit(x_train_tfidf, y_train)
y_pred_tfidf = nb_tfidf.predict(x_test_tfidf)

print("Naive Bayes with TF-IDF:")
print(classification_report(y_test, y_pred_tfidf))

# Accuracy
accuracy = accuracy_score(y_test, y_pred_tfidf)

# F1 Score
f1 = f1_score(y_test, y_pred_tfidf)

# Macro F1 Score
macro_f1 = f1_score(y_test, y_pred_tfidf, average='macro')

# Print results
print(f"Accuracy: {accuracy:.4f}")
print(f"F1 Score: {f1:.4f}")
print(f"Macro F1 Score: {macro_f1:.4f}")

# Calculate the confusion matrix
conf_matrix = confusion_matrix(y_test, y_pred_tfidf)

# Print the confusion matrix
print("Confusion Matrix:")

# Create a heatmap using Seaborn
plt.figure(figsize=(6, 4))
sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=["Negative", "Positive"], yticklabels=["Negative", "Positive"])

# Set labels and title
plt.title('Confusion Matrix')
plt.xlabel('Predicted Label')
plt.ylabel('True Label')

# Display the heatmap
plt.show()

# Store results
models.append("Naive Bayes with TF-IDF")
confusion_matrices.append(confusion_matrix(y_test, y_pred_tfidf))

# N-grams (Bigrams)
vectorizer_bigrams = CountVectorizer(ngram_range=(2, 2))
x_train_bigrams = vectorizer_bigrams.fit_transform(x_train)
x_test_bigrams = vectorizer_bigrams.transform(x_test)

# Naive Bayes with Bigrams
nb_bigrams = MultinomialNB()
nb_bigrams.fit(x_train_bigrams, y_train)
y_pred_bigrams = nb_bigrams.predict(x_test_bigrams)

print("Naive Bayes with Bigrams:")
print(classification_report(y_test, y_pred_bigrams))

# Accuracy
accuracy = accuracy_score(y_test, y_pred_bigrams)

# F1 Score
f1 = f1_score(y_test, y_pred_bigrams)

# Macro F1 Score
macro_f1 = f1_score(y_test, y_pred_bigrams, average='macro')

# Print the results
print(f"Accuracy: {accuracy:.4f}")
print(f"F1 Score: {f1:.4f}")
print(f"Macro F1 Score: {macro_f1:.4f}")

# Calculate the confusion matrix
conf_matrix = confusion_matrix(y_test, y_pred_bigrams)

# Print the confusion matrix
print("Confusion Matrix:")

# Create a heatmap using Seaborn
plt.figure(figsize=(6, 4))
sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=["Negative", "Positive"], yticklabels=["Negative", "Positive"])

# Set labels and title
plt.title('Confusion Matrix')
plt.xlabel('Predicted Label')
plt.ylabel('True Label')

# Display the heatmap
plt.show()

# Store results
models.append("Naive Bayes with Bigrams")
confusion_matrices.append(confusion_matrix(y_test, y_pred_bigrams))

# Decision Tree with BOW
vectorizer_bow = CountVectorizer()
x_train_bow = vectorizer_bow.fit_transform(x_train)
x_test_bow = vectorizer_bow.transform(x_test)

# Train Decision Tree classifier
dt_bow = DecisionTreeClassifier()
dt_bow.fit(x_train_tfidf, y_train)

# Make predictions on the test set
y_pred_dt_bow = dt_bow.predict(x_test_bow)

# Classification report
print("Decision Tree with Bag of Words:")
print(classification_report(y_test, y_pred_dt_bow))

# Calculate Accuracy
accuracy = accuracy_score(y_test, y_pred_tfidf)

# F1 Score
f1 = f1_score(y_test, y_pred_dt_bow)

# Macro F1 Score
macro_f1 = f1_score(y_test, y_pred_dt_bow, average='macro')

# Print results
print(f"Accuracy: {accuracy:.4f}")
print(f"F1 Score: {f1:.4f}")
print(f"Macro F1 Score: {macro_f1:.4f}")

# Calculate the confusion matrix
conf_matrix = confusion_matrix(y_test, y_pred_dt_bow)

# Print the confusion matrix
print("Confusion Matrix:")

# Create a heatmap using Seaborn
plt.figure(figsize=(6, 4))
sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=["Negative", "Positive"], yticklabels=["Negative", "Positive"])

# Set labels and title
plt.title('Confusion Matrix')
plt.xlabel('Predicted Label')
plt.ylabel('True Label')

# Display the heatmap
plt.show()

# Store results
models.append("Decision Tree with BoW")
confusion_matrices.append(confusion_matrix(y_test, y_pred_dt_bow))

# Decision Tree with TF-IDF
vectorizer_tfidf = TfidfVectorizer()
x_train_tfidf = vectorizer_tfidf.fit_transform(x_train)
x_test_tfidf = vectorizer_tfidf.transform(x_test)

# Decision Tree classifier with TF-IDF
dt_tfidf = DecisionTreeClassifier()
dt_tfidf.fit(x_train_tfidf, y_train)
y_pred_dt_tfidf = dt_tfidf.predict(x_test_tfidf)
print("Decision Tree with TF-IDF:")
print(classification_report(y_test, y_pred_dt_tfidf))

# Accuracy
accuracy = accuracy_score(y_test, y_pred_dt_tfidf)

# F1 Score
f1 = f1_score(y_test, y_pred_dt_tfidf)

# Macro F1 Score
macro_f1 = f1_score(y_test, y_pred_dt_tfidf, average='macro')

# Print the results
print(f"Accuracy: {accuracy:.4f}")
print(f"F1 Score: {f1:.4f}")
print(f"Macro F1 Score: {macro_f1:.4f}")

# Calculate the confusion matrix
conf_matrix = confusion_matrix(y_test, y_pred_dt_tfidf)

# Print the confusion matrix
print("Confusion Matrix:")

# Create a heatmap using Seaborn
plt.figure(figsize=(6, 4))
sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=["Negative", "Positive"], yticklabels=["Negative", "Positive"])

# Set labels and title
plt.title('Confusion Matrix')
plt.xlabel('Predicted Label')
plt.ylabel('True Label')

# Display the heatmap
plt.show()

# Store results
models.append("Decision Tree with TF-IDF")
confusion_matrices.append(confusion_matrix(y_test, y_pred_dt_tfidf))

# Decision Tree with Bigrams
vectorizer_dt_bigrams = CountVectorizer(ngram_range=(2, 2))
x_train_dt_bigrams = vectorizer_dt_bigrams.fit_transform(x_train)
x_test_dt_bigrams = vectorizer_dt_bigrams.transform(x_test)

dt_bigrams = DecisionTreeClassifier()
dt_bigrams.fit(x_train_dt_bigrams, y_train)
y_pred_dt_bigrams = dt_bigrams.predict(x_test_dt_bigrams)

print("Decision Tree with Bigrams:")
print(classification_report(y_test, y_pred_dt_bigrams))

# Calculate Accuracy
accuracy = accuracy_score(y_test, y_pred_dt_bigrams)

# Calculate F1 Score (default is binary)
f1 = f1_score(y_test, y_pred_dt_bigrams)

# Calculate Macro F1 Score (average='macro' means all classes are treated equally)
macro_f1 = f1_score(y_test, y_pred_tfidf, average='macro')

# Print the results
print(f"Accuracy: {accuracy:.4f}")
print(f"F1 Score: {f1:.4f}")
print(f"Macro F1 Score: {macro_f1:.4f}")

# Calculate the confusion matrix
conf_matrix = confusion_matrix(y_test, y_pred_dt_bigrams)

# Print the confusion matrix
print("Confusion Matrix:")

# Create a heatmap using Seaborn
plt.figure(figsize=(6, 4))
sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=["Negative", "Positive"], yticklabels=["Negative", "Positive"])

# Set labels and title
plt.title('Confusion Matrix')
plt.xlabel('Predicted Label')
plt.ylabel('True Label')

# Display the heatmap
plt.show()

# Store results
models.append("Decision Tree with Bigrams")
confusion_matrices.append(confusion_matrix(y_test, y_pred_dt_bigrams))

# SVM with Bag of Words (BoW)
vectorizer_bow = CountVectorizer()
x_train_bow = vectorizer_bow.fit_transform(x_train)
x_test_bow = vectorizer_bow.transform(x_test)

svm_bow = SVC()
svm_bow.fit(x_train_bow, y_train)
y_pred_svm_bow = svm_bow.predict(x_test_bow)

# Print classification report for SVM with BoW
print("SVM with BoW:")
print(classification_report(y_test, y_pred_svm_bow))

# Calculate Accuracy
accuracy = accuracy_score(y_test, y_pred_svm_bow)

# F1 Score
f1 = f1_score(y_test, y_pred_svm_bow)

# Macro F1 Score
macro_f1 = f1_score(y_test, y_pred_svm_bow, average='macro')

# Print the results
print(f"Accuracy: {accuracy:.4f}")
print(f"F1 Score: {f1:.4f}")
print(f"Macro F1 Score: {macro_f1:.4f}")

# Calculate the confusion matrix
conf_matrix = confusion_matrix(y_test, y_pred_svm_bow)

# Print the confusion matrix
print("Confusion Matrix:")

# Create a heatmap using Seaborn
plt.figure(figsize=(6, 4))
sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=["Negative", "Positive"], yticklabels=["Negative", "Positive"])

# Set labels and title
plt.title('Confusion Matrix')
plt.xlabel('Predicted Label')
plt.ylabel('True Label')

# Display the heatmap
plt.show()

# Store results
models.append("SVM with BoW")
confusion_matrices.append(confusion_matrix(y_test, y_pred_svm_bow))

# SVM with TF-IDF features
vectorizer_tfidf = TfidfVectorizer()
x_train_tfidf = vectorizer_tfidf.fit_transform(x_train)
x_test_tfidf = vectorizer_tfidf.transform(x_test)

svm_tfidf = SVC()
svm_tfidf.fit(x_train_tfidf, y_train)

y_pred_svm_tfidf = svm_tfidf.predict(x_test_tfidf)

# Print classification report for SVM with TF-IDF
print("SVM with TF-IDF:")
print(classification_report(y_test, y_pred_svm_tfidf))

# Calculate Accuracy
accuracy = accuracy_score(y_test, y_pred_svm_tfidf)

# Calculate F1 Score (default is binary)
f1 = f1_score(y_test, y_pred_svm_tfidf)

# Calculate Macro F1 Score (average='macro' means all classes are treated equally)
macro_f1 = f1_score(y_test, y_pred_svm_tfidf, average='macro')

# Print the results
print(f"Accuracy: {accuracy:.4f}")
print(f"F1 Score: {f1:.4f}")
print(f"Macro F1 Score: {macro_f1:.4f}")

# Calculate the confusion matrix
conf_matrix = confusion_matrix(y_test, y_pred_svm_tfidf)

# Print the confusion matrix
print("Confusion Matrix:")

# Create a heatmap using Seaborn
plt.figure(figsize=(6, 4))
sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=["Negative", "Positive"], yticklabels=["Negative", "Positive"])

# Set labels and title
plt.title('Confusion Matrix')
plt.xlabel('Predicted Label')
plt.ylabel('True Label')

# Display the heatmap
plt.show()

# Store results
models.append("SVM with TF-IDF")
confusion_matrices.append(confusion_matrix(y_test, y_pred_svm_tfidf))

#  SVM Classifier with N-grams (Bigrams)
vectorizer_svm_bigrams = CountVectorizer(ngram_range=(2, 2))
x_train_svm_bigrams = vectorizer_svm_bigrams.fit_transform(x_train)
x_test_svm_bigrams = vectorizer_svm_bigrams.transform(x_test)

svm_bigrams = SVC()
svm_bigrams.fit(x_train_svm_bigrams, y_train)
y_pred_svm_bigrams = svm_bigrams.predict(x_test_svm_bigrams)

print("SVM with Bigrams:")
print(classification_report(y_test, y_pred_svm_bigrams))

# Accuracy
accuracy = accuracy_score(y_test, y_pred_svm_bigrams)

# F1 Score
f1 = f1_score(y_test, y_pred_svm_bigrams)

# Macro F1 Score
macro_f1 = f1_score(y_test, y_pred_svm_bigrams, average='macro')

# Print results
print(f"Accuracy: {accuracy:.4f}")
print(f"F1 Score: {f1:.4f}")
print(f"Macro F1 Score: {macro_f1:.4f}")

# Calculate the confusion matrix
conf_matrix = confusion_matrix(y_test, y_pred_svm_bigrams)

# Print the confusion matrix
print("Confusion Matrix:")

# Create a heatmap using Seaborn
plt.figure(figsize=(6, 4))
sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=["Negative", "Positive"], yticklabels=["Negative", "Positive"])

# Set labels and title
plt.title('Confusion Matrix')
plt.xlabel('Predicted Label')
plt.ylabel('True Label')

# Display the heatmap
plt.show()

# Store results
models.append("SVM with Bigrams")
confusion_matrices.append(confusion_matrix(y_test, y_pred_svm_bigrams))

# Confusion Matrices for All Models

fig, axes = plt.subplots(3, 3, figsize=(18, 15))
axes = axes.ravel()  # Flatten axes array for easier indexing

for i, cm in enumerate(confusion_matrices):
    sns.heatmap(cm, annot=True, fmt='d', ax=axes[i], cmap='Blues', cbar=False)
    axes[i].set_title(f"Confusion Matrix - {models[i]}")
    axes[i].set_xlabel('Predicted')
    axes[i].set_ylabel('Actual')

plt.tight_layout()
plt.show()

